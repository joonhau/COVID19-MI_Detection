{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "import os,cv2\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.feature import canny\n",
    "from skimage.filters import roberts, farid, sobel, scharr, prewitt, laplace\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import area_closing, disk, dilation, binary_erosion, remove_small_objects, erosion, opening, closing, reconstruction, binary_closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "PATH = os.getcwd()\n",
    "data_path = PATH + '/CT'\n",
    "data_dir_list = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows=224\n",
    "img_cols=224\n",
    "num_channel=1\n",
    "\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 2\n",
    "labels_name={'Covid':0,'non-Covid':1}\n",
    "\n",
    "img_data_list=[]\n",
    "labels_list = []\n",
    "moment_list = []\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    label_data = labels_name[dataset]\n",
    "    for img in img_list:\n",
    "        im3= cv2.imread(data_path + '/'+ dataset + '/'+ img , 0)\n",
    "        im2= cv2.imread(data_path + '/'+ dataset + '/'+ img , 0)\n",
    "        im= cv2.imread(data_path + '/'+ dataset + '/'+ img , 0)\n",
    "        ret1,th1 = cv2.threshold(im, 127,255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        h,w = im.shape[0:2]\n",
    "        mask = np.zeros(im.shape, dtype=np.uint8)\n",
    "\n",
    "        left = int(np.round(h/(2)))\n",
    "        right = int(np.round(h/(2)))\n",
    "        for y in range(left-int(h/5.2), left+int(h/3.9)):\n",
    "            for x in range(0, 4):\n",
    "                im[y][x] = 255\n",
    "\n",
    "        for y in range(right-int(h/5.2), right+int(h/3.9)):\n",
    "                for x in range(w-4, w):\n",
    "                    im[y][x] = 255\n",
    "        \n",
    "        ret1,im2 = cv2.threshold(im2,180,180, cv2.THRESH_TOZERO)   \n",
    "        ## Binary image ##\n",
    "        binary = im < 150\n",
    "\n",
    "        ## Remove the blobs connected to the border of the image ##\n",
    "        cleared = clear_border(binary)\n",
    "\n",
    "        ## Label image ##\n",
    "        label_image = label(cleared)\n",
    "\n",
    "\n",
    "        ## Keep the labels with 2 largest areas ##\n",
    "        areas = [r.area for r in regionprops(label_image)]\n",
    "        areas.sort()\n",
    "\n",
    "\n",
    "        if len(areas) > 2:\n",
    "            for region in regionprops(label_image):\n",
    "                if region.area <= 5:\n",
    "                    for coordinates in region.coords: \n",
    "                        label_image[coordinates[0], coordinates[1]] = 0\n",
    "                        mask[coordinates[0], coordinates[1]] = 255\n",
    "\n",
    "\n",
    "        large_binary = label_image > 0  \n",
    "\n",
    "        ## Erosion operation with a disk of radius 2. This operation is seperate the lung nodules attached to the blood vessels ##\n",
    "        selem = disk(0)\n",
    "        binary_eroded = binary_erosion(large_binary, selem)\n",
    "\n",
    "\n",
    "        ## Closure operation with a disk of radius 10. This operation is to keep nodules attached to the lung wall ##\n",
    "        selem = disk(10)\n",
    "        binary_closed = binary_closing(binary_eroded, selem)\n",
    "\n",
    "        start_point = (0, 0) \n",
    "        end_point = (0,h) \n",
    "        color = (0,0,0) \n",
    "        binary_closed = cv2.line(np.float32(binary_closed), start_point, end_point, color, 5)\n",
    "\n",
    "        start_point2 = (w, 0) \n",
    "        end_point2 = (w,h) \n",
    "        binary_closed = cv2.line(np.float32(binary_closed), start_point2, end_point2, color, 5)\n",
    "\n",
    "        start_point2 = (0, h) \n",
    "        end_point2 = (w,h) \n",
    "        binary_closed = cv2.line(np.float32(binary_closed), start_point2, end_point2, color, 5)\n",
    "\n",
    "        start_point2 = (0, 0) \n",
    "        end_point2 = (w,0) \n",
    "        binary_closed = cv2.line(np.float32(binary_closed), start_point2, end_point2, color, 5)\n",
    "\n",
    "\n",
    "        ## Fill in the small holes inside the binary mask of lungs ##\n",
    "        edges = roberts(binary_closed)\n",
    "        hole_closed = ndi.binary_fill_holes(edges)\n",
    "\n",
    "        ## Superimpose the binary mask on the input image. ##\n",
    "        ZERO_VALUE = 0\n",
    "        get_high_vals = (hole_closed == 0)\n",
    "        im[get_high_vals] = ZERO_VALUE # minimum value\n",
    "        \n",
    "                \n",
    "        input_img_resize=cv2.resize(im,(224,224))\n",
    "        input_img_resize_gray = cv2.resize(im3,(224,224))\n",
    "        moments = cv2.moments(input_img_resize_gray)\n",
    "        huMoments = cv2.HuMoments(moments)   \n",
    "        moment_list.append(huMoments)\n",
    "        img_data_list.append(input_img_resize)\n",
    "        labels_list.append(label_data)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image normalization ##\n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "moment_data = np.array(moment_list)\n",
    "moment_data = moment_data.astype('float32')\n",
    "moment_data /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label one hot encoding ##\n",
    "labels = np.array(labels_list)\n",
    "print(np.unique(labels,return_counts=True))\n",
    "Y = np_utils.to_categorical(labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Colour channel expansion ##\n",
    "if num_channel==1:\n",
    "    if K.common.image_dim_ordering()=='th':\n",
    "        img_data= np.expand_dims(img_data, axis=1) \n",
    "        moment_data= np.expand_dims(moment_data, axis=1) \n",
    "        print (img_data.shape)\n",
    "    else:\n",
    "        img_data= np.expand_dims(img_data, axis=3) \n",
    "        moment_data= np.expand_dims(moment_data, axis=3) \n",
    "        print (img_data.shape)\n",
    "        print (moment_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataset\n",
    "x,y = shuffle(img_data,Y, random_state=2)\n",
    "x2,y2 = shuffle(moment_data,Y, random_state=2)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=2)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.1, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert train and test set to RGB\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_train = tf.image.grayscale_to_rgb(X_train)\n",
    "\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "X_test = tf.image.grayscale_to_rgb(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_tensor_train = tf.make_tensor_proto(X_train)\n",
    "X_train = tf.make_ndarray(proto_tensor_train)\n",
    "print(X_train.shape)\n",
    "\n",
    "proto_tensor_val = tf.make_tensor_proto(X_test)\n",
    "X_test = tf.make_ndarray(proto_tensor_val)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization of pre-trained ResNet50\n",
    "from keras.applications import ResNet50\n",
    "\n",
    "# include top should be False to remove the softmax layer\n",
    "pretrained_model = ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "output = pretrained_model.layers[-1].output\n",
    "output = keras.layers.GlobalAveragePooling2D()(output)\n",
    "pretrained_model = keras.Model(pretrained_model.input, output)\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features extraction ##\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "vgg_features_train = pretrained_model.predict(X_train)\n",
    "vgg_features_val = pretrained_model.predict(X_test)\n",
    "\n",
    "vgg_features_train = vgg_features_train.reshape(2052,2048,1)\n",
    "vgg_features_val = vgg_features_val.reshape(228,2048,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape moment invariant (training) values into 1 dimension\n",
    "X_train2_1 = X_train2\n",
    "Moment_Inv = X_train2_1.reshape(2052,7,1)\n",
    "Resnet_MI = K.concatenate([vgg_features_train, Moment_Inv],axis=1)\n",
    "\n",
    "# Reshape moment invariant (testing) values into 1 dimension\n",
    "X_test2_1 = X_test2\n",
    "Moment_Inv_val = X_test2_1.reshape(228,7,1)\n",
    "Resnet_MI_val = K.concatenate([vgg_features_val, Moment_Inv_val],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "proto_tensor = tf.make_tensor_proto(Resnet_MI)\n",
    "input_array = tf.make_ndarray(proto_tensor)\n",
    "input_array = input_array.reshape(2052, 2055,1)\n",
    "input_array_shape = input_array[0].shape\n",
    "print(input_array_shape.shape)\n",
    "\n",
    "proto_tensor_val = tf.make_tensor_proto(Resnet_MI_val)\n",
    "input_array_val = tf.make_ndarray(proto_tensor_val)\n",
    "input_array_val = input_array_val.reshape(228, 2055,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout, GlobalAveragePooling2D, Activation\n",
    "\n",
    "# Fully Connected layer\n",
    "model_fusion = Sequential()\n",
    "model_fusion.add(Flatten(input_shape=(2055,1)))\n",
    "model_fusion.add(Dense(256, activation='relu'))\n",
    "model_fusion.add(Dropout(0.5))\n",
    "model_fusion.add(BatchNormalization())\n",
    "model_fusion.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# sgd = SGD(lr=0.01, decay= 1e-8, momentum= 0.4, nesterov=True)\n",
    "# model_fusion.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "# sgd = SGD(lr=0.00004, decay= 1e-7, momentum= 0.4, nesterov=True)\n",
    "# model_fusion.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "anne = ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, patience=5, verbose=1, min_lr=1e-3)\n",
    "checkpoint = ModelCheckpoint('model/Resnet_Segment_MI.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "model_fusion.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=0.0004, decay= 1e-5), metrics=['accuracy'])\n",
    "model_fusion.summary()\n",
    "Fusion = model_fusion.fit(input_array, y_train, batch_size = 224 , epochs=100, verbose=1, validation_data=(input_array_val, y_test),callbacks=[anne, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving training acc and loss to CSV\n",
    "hist_df = pd.DataFrame(hist.history) \n",
    "\n",
    "hist_csv_file = 'history/ResNet_MI_Seg_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "    \n",
    "history = pd.read_csv(\"history/ResNet_MI_Seg_history.csv\")\n",
    "val_loss = history['val_loss'].to_numpy()\n",
    "val_acc = history['val_accuracy'].to_numpy()\n",
    "loss = history['loss'].to_numpy()\n",
    "acc = history['accuracy'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Acc vs Loss ##\n",
    "plt.plot(acc)\n",
    "plt.plot(loss)\n",
    "plt.plot(val_acc)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Training Accuracy vs Epochs (ResNet+MI+Segmentation) ', fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylabel('Accuracy (%)', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xlabel('Epoch',fontsize=20)\n",
    "plt.legend(['Train_acc', 'Train_loss', 'Val_acc', 'Val_loss'], loc='best', fontsize=22)\n",
    "plt.gcf().set_size_inches(12,10) \n",
    "#plt.savefig('C:/Users/CooL/Desktop/Result/All Training vs Loss/ResNet_MI_train_graph.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing phase ###\n",
    "\n",
    "PATH = os.getcwd()\n",
    "# Define data path\n",
    "data_path = PATH + '/Testing_Set'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt   \n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "moment_list2 = []\n",
    "test_list = []\n",
    "y_pred = []\n",
    "labels_name ={'Covid':0,'non-Covid':1}\n",
    "\n",
    "#ret,thresh3 = cv2.threshold(input_img,200,255,cv2.THRESH_TRUNC)\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    label_data = labels_name[dataset]\n",
    "    for img in img_list:\n",
    "        im = cv2.imread(data_path + '/'+ dataset + '/'+ img, 0 )\n",
    "        im3 = cv2.imread(data_path + '/'+ dataset + '/'+ img, 0 )\n",
    "        h,w = im.shape[0:2]\n",
    "        mask = np.zeros(im.shape, dtype=np.uint8)\n",
    "\n",
    "        left = int(np.round(h/(2)))\n",
    "        right = int(np.round(h/(2)))\n",
    "        for y in range(left-int(h/5.2), left+int(h/3.9)):\n",
    "            for x in range(0, 4):\n",
    "                im[y][x] = 255\n",
    "\n",
    "        for y in range(right-int(h/5.2), right+int(h/3.9)):\n",
    "                for x in range(w-4, w):\n",
    "                    im[y][x] = 255\n",
    "\n",
    "        ## Binary image ##\n",
    "        binary = im < 150\n",
    "\n",
    "        ## Remove the blobs connected to the border of the image ##\n",
    "        cleared = clear_border(binary)\n",
    "\n",
    "        ## Label image ##\n",
    "        label_image = label(cleared)\n",
    "\n",
    "\n",
    "        ## Keep the labels with 2 largest areas ##\n",
    "        areas = [r.area for r in regionprops(label_image)]\n",
    "        areas.sort()\n",
    "\n",
    "\n",
    "        if len(areas) > 2:\n",
    "            for region in regionprops(label_image):\n",
    "                if region.area <= 5:\n",
    "                    for coordinates in region.coords: \n",
    "                        label_image[coordinates[0], coordinates[1]] = 0\n",
    "                        mask[coordinates[0], coordinates[1]] = 255\n",
    "\n",
    "\n",
    "        large_binary = label_image > 0  \n",
    "\n",
    "        ## Erosion operation with a disk of radius 2. This operation is seperate the lung nodules attached to the blood vessels ##\n",
    "        selem = disk(0)\n",
    "        binary_eroded = binary_erosion(large_binary, selem)\n",
    "\n",
    "\n",
    "        ## Closure operation with a disk of radius 10. This operation is to keep nodules attached to the lung wall ##\n",
    "        selem = disk(10)\n",
    "        binary_closed = binary_closing(binary_eroded, selem)\n",
    "\n",
    "        start_point = (0, 0) \n",
    "        end_point = (0,h) \n",
    "        color = (0,0,0) \n",
    "        binary_closed = cv2.line(np.float32(binary_closed), start_point, end_point, color, 5)\n",
    "\n",
    "        start_point2 = (w, 0) \n",
    "        end_point2 = (w,h) \n",
    "        binary_closed = cv2.line(np.float32(binary_closed), start_point2, end_point2, color, 5)\n",
    "\n",
    "        start_point2 = (0, h) \n",
    "        end_point2 = (w,h) \n",
    "        binary_closed = cv2.line(np.float32(binary_closed), start_point2, end_point2, color, 5)\n",
    "\n",
    "        start_point2 = (0, 0) \n",
    "        end_point2 = (w,0) \n",
    "        binary_closed = cv2.line(np.float32(binary_closed), start_point2, end_point2, color, 5)\n",
    "\n",
    "\n",
    "        ## Fill in the small holes inside the binary mask of lungs ##\n",
    "        edges = roberts(binary_closed)\n",
    "        hole_closed = ndi.binary_fill_holes(edges)\n",
    "\n",
    "        ## Superimpose the binary mask on the input image. ##\n",
    "        ZERO_VALUE = 0\n",
    "        get_high_vals = (hole_closed == 0)\n",
    "        im[get_high_vals] = ZERO_VALUE # minimum value\n",
    "        \n",
    "        input_img_resize = cv2.resize(im,(224,224))\n",
    "        input_img_resize_gray = cv2.resize(im3,(224,224))\n",
    "        moments = cv2.moments(input_img_resize_gray)\n",
    "        huMoments = cv2.HuMoments(moments)   \n",
    "        moment_list2.append(huMoments)\n",
    "        test_list.append(input_img_resize)\n",
    "        y_pred.append(label_data)\n",
    "\n",
    "\n",
    "print(len(moment_list2))\n",
    "test_image = np.array(test_list)\n",
    "test_image = test_image.astype('float32')\n",
    "test_image /= 255\n",
    "moment_data = np.array(moment_list2)\n",
    "moment_data = moment_data.astype('float32')\n",
    "moment_data /= 255\n",
    "\n",
    "\n",
    "if num_channel==1:\n",
    "    if K.common.image_dim_ordering()=='th':\n",
    "        test_image= np.expand_dims(test_image, axis=1) \n",
    "        moment_data= np.expand_dims(moment_data, axis=1) \n",
    "        print (test_image.shape)\n",
    "    else:\n",
    "        test_image= np.expand_dims(test_image, axis=3) \n",
    "        moment_data= np.expand_dims(moment_data, axis=3) \n",
    "        print (test_image.shape)\n",
    "        print (moment_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Convert train and test set to RGB\n",
    "test_image = tf.convert_to_tensor(test_image)\n",
    "test_image = tf.image.grayscale_to_rgb(test_image)\n",
    "\n",
    "proto_tensor_val = tf.make_tensor_proto(test_image)\n",
    "test_image = tf.make_ndarray(proto_tensor_val)\n",
    "\n",
    "test_features = pretrained_model.predict(test_image)\n",
    "test_features = test_features.reshape(200,2048,1)\n",
    "\n",
    "# Reshape moment invariant (testing) values into 1 dimension\n",
    "test_data = moment_data\n",
    "test_data = test_data.reshape(200,7,1)\n",
    "Test_MI = K.concatenate([test_features, test_data],axis=1)\n",
    "\n",
    "proto_tensor = tf.make_tensor_proto(Test_MI)\n",
    "input_array_test = tf.make_ndarray(proto_tensor)\n",
    "input_array_test = input_array_test.reshape(200, 2055, 1)\n",
    "\n",
    "\n",
    "\n",
    "predict_list = []\n",
    "predict_1 = model_fusion.predict(input_array_test)\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_pred, predict_1[:,1])\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "import pandas as pd\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "optimum = thresholds_keras[np.argmax(tpr_keras - fpr_keras)]\n",
    "print('Optimal cut off point = ',optimum)\n",
    "print('AUC = ', auc_keras)\n",
    "\n",
    "gmeans = np.sqrt(tpr_keras * (1-fpr_keras))\n",
    "ix = np.argmax(gmeans)\n",
    "optimal = thresholds_keras[ix]\n",
    "print('Optimal cut off point = ',thresholds_keras[ix])\n",
    "\n",
    "x = optimal # Best => x = 0.98892\n",
    "\n",
    "for j in range(0,1):\n",
    "    for i in range(0, len(predict_1)):\n",
    "        if predict_1[i][1] >= x:\n",
    "            predict_list.append(1)\n",
    "        else:\n",
    "            predict_list.append(0)\n",
    "    print(confusion_matrix(y_pred, predict_list), 'x =',\"%.5f\" % x, '\\n')\n",
    "    \n",
    "\n",
    "#### Confusion matrix\n",
    "cf_matrix = confusion_matrix(y_pred, predict_list)\n",
    "\n",
    "tp = cf_matrix[0][0]\n",
    "fn = cf_matrix[0][1]\n",
    "fp = cf_matrix[1][0]\n",
    "tn = cf_matrix[1][1]\n",
    "\n",
    "TrueP = '{0:.2%}'.format(tp/(tp+fn))\n",
    "FalseN = '{0:.2%}'.format(fn/(tp+fn))\n",
    "FalseP = '{0:.2%}'.format(fp/(fp+tn))\n",
    "TrueN = '{0:.2%}'.format(tn/(fp+tn))\n",
    "print(TrueP, FalseN, FalseP, TrueN)\n",
    "\n",
    "xy_Labels = ['Covid', 'non-Covid']\n",
    "group_names = ['True Positive', 'False Negative', 'False Positive','True Negative']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "group_percentage = [TrueP, FalseN, FalseP, TrueN]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentage)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "### Confusion Matrix (ResNet + MI + Segmentation) ###\n",
    "plt.title('(Segmentation + Moment Invariant + ResNet-50)', fontsize = 15)\n",
    "sns.heatmap(cf_matrix, fmt='', annot=labels, annot_kws={'size':18} ,xticklabels=xy_Labels, yticklabels=xy_Labels, cmap='RdPu')\n",
    "plt.xlabel('Predicted label', fontsize = 15) # x-axis label with fontsize 15\n",
    "plt.ylabel('True label', fontsize = 15) # y-axis label with fontsize 15\n",
    "plt.savefig('C:/Users/CooL/Desktop/Result/Resnet_segmented/Moment+ Segmentation/Resnet_Seg_MI_CF.jpg')\n",
    "plt.show()\n",
    "\n",
    "### ROC Curves (ResNet + MI + Segmentation) ###\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.scatter(fpr_keras[ix],tpr_keras[ix], marker='o', color='black', label='Optimal Threshold = {:.4f})'.format(optimal))\n",
    "plt.xlabel('False positive rate (FPR)', fontsize=15)\n",
    "plt.ylabel('True positive rate (TPR)', fontsize=15)\n",
    "plt.title('ROC (ResNet-50 + Segmentaion + MI)',fontsize=15)\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.savefig('C:/Users/CooL/Desktop/Result/Resnet_segmented/Moment+ Segmentation/Resnet_Seg_MI_graph.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
